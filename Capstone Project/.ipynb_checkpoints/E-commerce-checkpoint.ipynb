{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PGP AI - AI and Machine Learning Capstone Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DESCRIPTION\n",
    "\n",
    "Problem Statement\n",
    "\n",
    "Amazon is an online shopping website that now caters to millions of people everywhere. Over 34,000 consumer reviews for Amazon brand products like Kindle, Fire TV Stick and more are provided. \n",
    "The dataset has attributes like brand, categories, primary categories, reviews.title, reviews.text, and the sentiment. Sentiment is a categorical variable with three levels \"Positive\", \"Negativeâ€œ, and \"Neutral\". For a given unseen data, the sentiment needs to be predicted.\n",
    "You are required to predict Sentiment or Satisfaction of a purchase based on multiple features and review text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pprint import pprint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path=\"\"):\n",
    "    return pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(datasets=[]):\n",
    "    return pd.concat(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data(data, row_count=5):\n",
    "    return data[:row_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    data = [regexp_tokenize(record, pattern='[a-z]\\w+') \n",
    "                   for record in data]\n",
    "    print(\"Completed Word Tokenization\")\n",
    "\n",
    "    data = [[word for word in text \n",
    "                    if word not in stopwords.words('english')] \n",
    "                   for text in data]\n",
    "    print(\"Completed Removal of nltk StopWords\")\n",
    "\n",
    "    data = [[word for word in text \n",
    "                    if len(word) > 1] \n",
    "                   for text in data]\n",
    "    print(\"Completed Removal of Words of length = 1\")\n",
    "\n",
    "    data_cleaned = []\n",
    "\n",
    "    for record in data:\n",
    "        sent = \"\"\n",
    "        for word in record:\n",
    "            sent = sent + word + \" \"\n",
    "        data_cleaned.append(sent[:-1])\n",
    "\n",
    "    print(\"Completed Joining of the cleaned Text into a record\")\n",
    "    \n",
    "    print(\"Analysing Tokens:\")\n",
    "    token_analysis(data)\n",
    "    \n",
    "    return data_cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_analysis(data):\n",
    "    terms = []\n",
    "\n",
    "    for record in data:\n",
    "        for word in record:\n",
    "            terms.append(word)\n",
    "\n",
    "    print(\"Total Tokens: {}\".format(len(terms)))\n",
    "\n",
    "    from collections import Counter\n",
    "\n",
    "    counts_terms = Counter(terms)\n",
    "    terms_df = pd.DataFrame(counts_terms.most_common(10), \n",
    "                            columns=['term', 'count'])\n",
    "    terms_df\n",
    "\n",
    "    terms_df.sort_values(by='count', \n",
    "                         ascending=True).plot(kind=\"barh\", \n",
    "                                              x='term', \n",
    "                                              figsize=(12,10), \n",
    "                                              color='teal')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(data, features, label, vectorizer):\n",
    "    X = vectorizer.fit_transform(data[features].tolist())\n",
    "    y = data[label].tolist()\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def resample_dataset(X, y, sampler):\n",
    "    return sampler.fit_sample(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_test_datasets(X, y):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    return train_test_split(X, y, test_size=0.3, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(X, y, model):\n",
    "    return model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_model_score(model, X_test, y_test):\n",
    "    return f1_score(y_test, model.predict(X_test), average='weighted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = load_data(\"datasets/train_data.csv\")\n",
    "test_data_hidden_df = load_data(\"datasets/test_data_hidden.csv\")\n",
    "test_df = load_data(\"datasets/test_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()\n",
    "test_data_hidden_df.info()\n",
    "test_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df = merge_data([train_df,test_data_hidden_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_data(merge_df,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merge_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations:\n",
    "- Total Records: 5000\n",
    "- Data Column for building model: reviews.text\n",
    "- Label Column for identifying class: sentiment\n",
    "- Unique Reviews: 4385\n",
    "- Number of Output Class: 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(merge_df['sentiment'].value_counts())\n",
    "merge_df['sentiment'].value_counts().plot(kind='bar', title=\"Count(sentiment) for Training Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "merge_df['review_cleaned'] = preprocess(merge_df['reviews.text'].tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = create_dataset(merge_df, 'review_cleaned', 'sentiment', TfidfVectorizer(max_features=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(len(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating a Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df = test_df.copy()\n",
    "validation_df.insert(7, 'sentiment', test_data_hidden_df['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "validation_df['reviews_cleaned'] = preprocess(validation_df['reviews.text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(validation_df['sentiment'].value_counts())\n",
    "validation_df['sentiment'].value_counts().plot(kind='bar', title=\"Count(sentiment) for Test Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = create_dataset(validation_df, 'reviews_cleaned', 'sentiment', TfidfVectorizer(max_features=2000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)\n",
    "print(len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_nb = create_model(X_train, y_train, MultinomialNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, multi_nb.predict(X_test), digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over Sampled Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_os, y_train_os = resample_dataset(X_train, y_train, RandomOverSampler())\n",
    "print(X_train_os.shape)\n",
    "\n",
    "print(pd.DataFrame(y_train_os)[0].value_counts())\n",
    "plt.style.use('seaborn')\n",
    "pd.DataFrame(y_train_os)[0].value_counts().plot(kind='bar', title='Over Sampled Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Over Sampled Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_os, y_test_os = resample_dataset(X_test, y_test, RandomOverSampler())\n",
    "print(X_test_os.shape)\n",
    "\n",
    "print(pd.DataFrame(y_test_os)[0].value_counts())\n",
    "plt.style.use('seaborn')\n",
    "pd.DataFrame(y_test_os)[0].value_counts().plot(kind='bar', title='Over Sampled Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under Sampled Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_us, y_train_us = resample_dataset(X_train, ytrain, RandomUnderSampler())\n",
    "print(X_train_us.shape)\n",
    "\n",
    "print(pd.DataFrame(y_train_us)[0].value_counts())\n",
    "plt.style.use('seaborn')\n",
    "pd.DataFrame(y_train_us)[0].value_counts().plot(kind='bar', title='Under Sampled Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Under Sampled Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_us, y_test_us = resample_dataset(X_test, y_test, RandomUnderSampler())\n",
    "print(X_test_us.shape)\n",
    "\n",
    "print(pd.DataFrame(y_test_us)[0].value_counts())\n",
    "plt.style.use('seaborn')\n",
    "pd.DataFrame(y_test_us)[0].value_counts().plot(kind='bar', title='Under Sampled Data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "            'multinomial_nb': MultinomialNB(),\n",
    "            'random_forest': RandomForestClassifier(),\n",
    "            'XGBClassifer': XGBClassifier()\n",
    "         }\n",
    "\n",
    "samples = {\n",
    "            'unsampled': [X_train, y_train, X_test, y_test],\n",
    "            'over_sampled': [X_train_os, y_train_os, X_test_os, y_test_os],\n",
    "            'under_sampled': [X_train_us, y_train_us, X_test_us, y_test_us]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_models(models, samples):\n",
    "    scores = dict()\n",
    "    for model_name, model in models.items():\n",
    "        for label, sample in samples.items():\n",
    "            scores[model_name+'-'+label] = assess_model_score(create_model(sample[0], sample[1], model), sample[2], sample[3])\n",
    "    return scores        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "scores = pd.Series(create_models(models, samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores)\n",
    "scores.plot(kind='barh', title='F1 Scores', figsize=(15,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_kernels = {\n",
    "    'linear': {\n",
    "        'kernel':'linear', \n",
    "        'C':1, \n",
    "        'decision_function_shape':'ovo'\n",
    "    },\n",
    "    'rbf': {\n",
    "        'kernel':'rbf', \n",
    "        'C':1,\n",
    "        'gamma':1,\n",
    "        'decision_function_shape':'ovo'\n",
    "    },\n",
    "    'poly': {\n",
    "        'kernel':'poly', \n",
    "        'C':1,\n",
    "        'degree':3,\n",
    "        'decision_function_shape':'ovo'\n",
    "    },\n",
    "    'sigmoid': {\n",
    "        'kernel':'sigmoid', \n",
    "        'C':1, \n",
    "        'decision_function_shape':'ovo'\n",
    "    }\n",
    "}\n",
    "def assess_svm_kernels(samples):\n",
    "    svm_scores = dict()\n",
    "    for label, sample in samples.items():\n",
    "        for model_name, params in svm_kernels.items():\n",
    "            model = SVC().set_params(**params).fit(sample[0], sample[1])\n",
    "            svm_scores[model_name+'-'+label] = assess_model_score(model, sample[2], sample[3])\n",
    "    return svm_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "svm_scores = pd.Series(assess_svm_kernels(samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(svm_scores)\n",
    "svm_scores.plot(kind='barh', title='F1 Scores for SVM', figsize=(15,6))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
